{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119678\n",
      "119678\n",
      "39893\n",
      "39893\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "submission_df = pd.read_csv('../input/test.csv')\n",
    "\n",
    "train_comments_orig = train_df['comment_text']\n",
    "submission_comments = submission_df['comment_text']\n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train_comments, test_comments, train_true, test_true = train_test_split(train_comments_orig, train_df[labels])\n",
    "test_true_matrix = test_true.as_matrix()\n",
    "\n",
    "print(len(train_comments))\n",
    "print(len(train_true))\n",
    "print(len(test_comments))\n",
    "print(len(test_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentsEmbedder():\n",
    "    \n",
    "    def __init__(self, fit_comments):\n",
    "        self.fit_comments = fit_comments\n",
    "        self.num_words = 10000\n",
    "\n",
    "        self.vectorizer = vectorizer = TfidfVectorizer(\n",
    "                                        analyzer='word', \n",
    "                                        sublinear_tf=True,\n",
    "                                        strip_accents='unicode',\n",
    "                                        token_pattern=r'\\w{1,}',\n",
    "                                        stop_words='english',\n",
    "                                        ngram_range=(1, 3),\n",
    "                                        max_features=self.num_words)\n",
    "        self.tfidf = self.vectorizer.fit(self.fit_comments)\n",
    "        \n",
    "    '''transform array of comments to tfidf matrix'''\n",
    "    def transform(self, comments):\n",
    "        sparse = self.tfidf.transform(comments)\n",
    "        return sparse.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentData():\n",
    "    \n",
    "    def __init__(self, comments, y_true=None):\n",
    "        \n",
    "        self.comments = comments\n",
    "        self.y_true = y_true\n",
    "        self.i = 0\n",
    "        self.do_next_batch = True\n",
    "        \n",
    "        \n",
    "    \n",
    "    def next_batch(self,batch_size):\n",
    "        if self.i + batch_size >= len(self.comments):\n",
    "            new_i = len(self.comments) + 1\n",
    "            self.do_next_batch = False\n",
    "        else:\n",
    "            new_i = self.i + batch_size\n",
    "        \n",
    "        batch_x = self.comments[self.i:new_i]\n",
    "        \n",
    "        if self.y_true is not None:\n",
    "            batch_y = self.y_true[self.i:new_i].as_matrix()\n",
    "            self.i = new_i\n",
    "            return batch_x, batch_y    \n",
    "        else:\n",
    "            self.i = new_i\n",
    "            return batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "def conv1d(x, W):\n",
    "    # x is input tensor --> [batch, num_words, in_channels]\n",
    "    # W is the kernel --> [filter width, in_channels, out_channels]\n",
    "    return tf.nn.conv1d(x,W, stride=1, padding='SAME')\n",
    "\n",
    "def max_pool(x):\n",
    "    # x is input tensor --> [batch, num_words, in_channels]\n",
    "    return tf.nn.pool(x, window_shape=[1], pooling_type='MAX', padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[2]])\n",
    "    \n",
    "    return tf.nn.relu(conv1d(input_x, W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    \n",
    "    return tf.matmul(input_layer,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting vectorizer...\n"
     ]
    }
   ],
   "source": [
    "print('Fitting vectorizer...')\n",
    "        \n",
    "comment_embedder = CommentsEmbedder(train_comments_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment_matrix = comment_embedder.transform(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max possible number of iterations: 7978\n"
     ]
    }
   ],
   "source": [
    "batchSize = 15\n",
    "numClasses = len(labels)\n",
    "iterations = 10635\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print('Max possible number of iterations: {}'.format(int(len(train_comments)/batchSize)))\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, shape=[None, comment_embedder.num_words])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, numClasses])\n",
    "hold_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERS\n",
    "\n",
    "convo_1_num_features = 10\n",
    "convo_1_filter_width = 2\n",
    "\n",
    "convo_2_num_features = 20\n",
    "convo_2_filter_width = 2\n",
    "\n",
    "convo_input = tf.reshape(input_data, [-1, comment_embedder.num_words ,1]) \n",
    "\n",
    "convo_1 = convolutional_layer(convo_input, shape=[convo_1_filter_width, 1, convo_1_num_features])\n",
    "\n",
    "convo_1_pooling = max_pool(convo_1)\n",
    "\n",
    "convo_2 = convolutional_layer(convo_1_pooling, shape=[convo_2_filter_width, convo_1_num_features, convo_2_num_features])\n",
    "\n",
    "#convo_2 = convolutional_layer(convo_input, shape=[convo_2_filter_width, 1, convo_2_num_features])\n",
    "\n",
    "convo_2_pooling = max_pool(convo_2)\n",
    "\n",
    "convo_2_flat = tf.reshape(convo_2_pooling, shape=[-1, comment_embedder.num_words*convo_2_num_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPOUT\n",
    "\n",
    "dropout = tf.nn.dropout(convo_2_flat, keep_prob=hold_prob)\n",
    "\n",
    "\n",
    "normal_full = normal_full_layer(dropout, numClasses)\n",
    "y_pred = tf.sigmoid(normal_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "\n",
    "loss = tf.reduce_mean(tf.losses.log_loss(y_true, y_pred))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on step 0\n",
      "Test Set Loss:\n",
      "0.95637685\n",
      "\n",
      "\n",
      "Currently on step 100\n",
      "Test Set Loss:\n",
      "0.1740433\n",
      "\n",
      "\n",
      "Currently on step 200\n",
      "Test Set Loss:\n",
      "0.16933191\n",
      "\n",
      "\n",
      "Currently on step 300\n",
      "Test Set Loss:\n",
      "0.16312648\n",
      "\n",
      "\n",
      "Currently on step 400\n",
      "Test Set Loss:\n",
      "0.15473247\n",
      "\n",
      "\n",
      "Currently on step 500\n",
      "Test Set Loss:\n",
      "0.19235931\n",
      "\n",
      "\n",
      "Currently on step 600\n",
      "Test Set Loss:\n",
      "0.16499083\n",
      "\n",
      "\n",
      "Currently on step 700\n",
      "Test Set Loss:\n",
      "0.1462277\n",
      "\n",
      "\n",
      "Currently on step 800\n",
      "Test Set Loss:\n",
      "0.15115045\n",
      "\n",
      "\n",
      "Currently on step 900\n",
      "Test Set Loss:\n",
      "0.1337341\n",
      "\n",
      "\n",
      "Currently on step 1000\n",
      "Test Set Loss:\n",
      "0.15407379\n",
      "\n",
      "\n",
      "Currently on step 1100\n",
      "Test Set Loss:\n",
      "0.12881525\n",
      "\n",
      "\n",
      "Currently on step 1200\n",
      "Test Set Loss:\n",
      "0.14420582\n",
      "\n",
      "\n",
      "Currently on step 1300\n",
      "Test Set Loss:\n",
      "0.13893545\n",
      "\n",
      "\n",
      "Currently on step 1400\n",
      "Test Set Loss:\n",
      "0.14703564\n",
      "\n",
      "\n",
      "Currently on step 1500\n",
      "Test Set Loss:\n",
      "0.14093341\n",
      "\n",
      "\n",
      "Currently on step 1600\n",
      "Test Set Loss:\n",
      "0.14077774\n",
      "\n",
      "\n",
      "Currently on step 1700\n",
      "Test Set Loss:\n",
      "0.13782941\n",
      "\n",
      "\n",
      "Currently on step 1800\n",
      "Test Set Loss:\n",
      "0.14921567\n",
      "\n",
      "\n",
      "Currently on step 1900\n",
      "Test Set Loss:\n",
      "0.14362311\n",
      "\n",
      "\n",
      "Currently on step 2000\n",
      "Test Set Loss:\n",
      "0.14287208\n",
      "\n",
      "\n",
      "Currently on step 2100\n",
      "Test Set Loss:\n",
      "0.15021642\n",
      "\n",
      "\n",
      "Currently on step 2200\n",
      "Test Set Loss:\n",
      "0.13607073\n",
      "\n",
      "\n",
      "Currently on step 2300\n",
      "Test Set Loss:\n",
      "0.14273617\n",
      "\n",
      "\n",
      "Currently on step 2400\n",
      "Test Set Loss:\n",
      "0.1467481\n",
      "\n",
      "\n",
      "Currently on step 2500\n",
      "Test Set Loss:\n",
      "0.14988722\n",
      "\n",
      "\n",
      "Currently on step 2600\n",
      "Test Set Loss:\n",
      "0.13912277\n",
      "\n",
      "\n",
      "Currently on step 2700\n",
      "Test Set Loss:\n",
      "0.14070502\n",
      "\n",
      "\n",
      "Currently on step 2800\n",
      "Test Set Loss:\n",
      "0.1452464\n",
      "\n",
      "\n",
      "Currently on step 2900\n",
      "Test Set Loss:\n",
      "0.14614902\n",
      "\n",
      "\n",
      "Currently on step 3000\n",
      "Test Set Loss:\n",
      "0.14457493\n",
      "\n",
      "\n",
      "Currently on step 3100\n",
      "Test Set Loss:\n",
      "0.13832873\n",
      "\n",
      "\n",
      "Currently on step 3200\n",
      "Test Set Loss:\n",
      "0.15072109\n",
      "\n",
      "\n",
      "Currently on step 3300\n",
      "Test Set Loss:\n",
      "0.14118937\n",
      "\n",
      "\n",
      "Currently on step 3400\n",
      "Test Set Loss:\n",
      "0.14551581\n",
      "\n",
      "\n",
      "Currently on step 3500\n",
      "Test Set Loss:\n",
      "0.14089283\n",
      "\n",
      "\n",
      "Currently on step 3600\n",
      "Test Set Loss:\n",
      "0.14768146\n",
      "\n",
      "\n",
      "Currently on step 3700\n",
      "Test Set Loss:\n",
      "0.14951459\n",
      "\n",
      "\n",
      "Currently on step 3800\n",
      "Test Set Loss:\n",
      "0.14166282\n",
      "\n",
      "\n",
      "Currently on step 3900\n",
      "Test Set Loss:\n",
      "0.14958587\n",
      "\n",
      "\n",
      "Currently on step 4000\n",
      "Test Set Loss:\n",
      "0.144958\n",
      "\n",
      "\n",
      "Currently on step 4100\n",
      "Test Set Loss:\n",
      "0.14969008\n",
      "\n",
      "\n",
      "Currently on step 4200\n",
      "Test Set Loss:\n",
      "0.1334482\n",
      "\n",
      "\n",
      "Currently on step 4300\n",
      "Test Set Loss:\n",
      "0.13694577\n",
      "\n",
      "\n",
      "Currently on step 4400\n",
      "Test Set Loss:\n",
      "0.1458622\n",
      "\n",
      "\n",
      "Currently on step 4500\n",
      "Test Set Loss:\n",
      "0.15303712\n",
      "\n",
      "\n",
      "Currently on step 4600\n",
      "Test Set Loss:\n",
      "0.14961857\n",
      "\n",
      "\n",
      "Currently on step 4700\n",
      "Test Set Loss:\n",
      "0.13811003\n",
      "\n",
      "\n",
      "Currently on step 4800\n",
      "Test Set Loss:\n",
      "0.13906327\n",
      "\n",
      "\n",
      "Currently on step 4900\n",
      "Test Set Loss:\n",
      "0.15259509\n",
      "\n",
      "\n",
      "Currently on step 5000\n",
      "Test Set Loss:\n",
      "0.14649309\n",
      "\n",
      "\n",
      "Currently on step 5100\n",
      "Test Set Loss:\n",
      "0.13941489\n",
      "\n",
      "\n",
      "Currently on step 5200\n",
      "Test Set Loss:\n",
      "0.14574681\n",
      "\n",
      "\n",
      "Currently on step 5300\n",
      "Test Set Loss:\n",
      "0.14130637\n",
      "\n",
      "\n",
      "Currently on step 5400\n",
      "Test Set Loss:\n",
      "0.14052184\n",
      "\n",
      "\n",
      "Currently on step 5500\n",
      "Test Set Loss:\n",
      "0.12970082\n",
      "\n",
      "\n",
      "Currently on step 5600\n",
      "Test Set Loss:\n",
      "0.14596324\n",
      "\n",
      "\n",
      "Currently on step 5700\n",
      "Test Set Loss:\n",
      "0.14985113\n",
      "\n",
      "\n",
      "Currently on step 5800\n",
      "Test Set Loss:\n",
      "0.13996653\n",
      "\n",
      "\n",
      "Currently on step 5900\n",
      "Test Set Loss:\n",
      "0.14519313\n",
      "\n",
      "\n",
      "Currently on step 6000\n",
      "Test Set Loss:\n",
      "0.14147933\n",
      "\n",
      "\n",
      "Currently on step 6100\n",
      "Test Set Loss:\n",
      "0.15026507\n",
      "\n",
      "\n",
      "Currently on step 6200\n",
      "Test Set Loss:\n",
      "0.14794546\n",
      "\n",
      "\n",
      "Currently on step 6300\n",
      "Test Set Loss:\n",
      "0.1372008\n",
      "\n",
      "\n",
      "Currently on step 6400\n",
      "Test Set Loss:\n",
      "0.13524124\n",
      "\n",
      "\n",
      "Currently on step 6500\n",
      "Test Set Loss:\n",
      "0.13717984\n",
      "\n",
      "\n",
      "Currently on step 6600\n",
      "Test Set Loss:\n",
      "0.14065447\n",
      "\n",
      "\n",
      "Currently on step 6700\n",
      "Test Set Loss:\n",
      "0.14496471\n",
      "\n",
      "\n",
      "Currently on step 6800\n",
      "Test Set Loss:\n",
      "0.14395384\n",
      "\n",
      "\n",
      "Currently on step 6900\n",
      "Test Set Loss:\n",
      "0.13987002\n",
      "\n",
      "\n",
      "Currently on step 7000\n",
      "Test Set Loss:\n",
      "0.13351226\n",
      "\n",
      "\n",
      "Currently on step 7100\n",
      "Test Set Loss:\n",
      "0.16184013\n",
      "\n",
      "\n",
      "Currently on step 7200\n",
      "Test Set Loss:\n",
      "0.15037054\n",
      "\n",
      "\n",
      "Currently on step 7300\n",
      "Test Set Loss:\n",
      "0.1297212\n",
      "\n",
      "\n",
      "Currently on step 7400\n",
      "Test Set Loss:\n",
      "0.13137333\n",
      "\n",
      "\n",
      "Currently on step 7500\n",
      "Test Set Loss:\n",
      "0.13602489\n",
      "\n",
      "\n",
      "Currently on step 7600\n",
      "Test Set Loss:\n",
      "0.1433395\n",
      "\n",
      "\n",
      "Currently on step 7700\n",
      "Test Set Loss:\n",
      "0.14638041\n",
      "\n",
      "\n",
      "Currently on step 7800\n",
      "Test Set Loss:\n",
      "0.13644776\n",
      "\n",
      "\n",
      "Currently on step 7900\n",
      "Test Set Loss:\n",
      "0.14555427\n",
      "\n",
      "\n",
      "Finished training, making predictions...\n",
      "On prediction 30000\n",
      "On prediction 60000\n",
      "On prediction 90000\n",
      "On prediction 120000\n",
      "On prediction 150000\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    data = CommentData(train_comments, train_true)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        if data.do_next_batch == False:\n",
    "            break\n",
    "        \n",
    "        batch_x , batch_y = data.next_batch(batchSize)\n",
    "        \n",
    "        batch_x = comment_embedder.transform(batch_x)\n",
    "        \n",
    "        sess.run(train,feed_dict={input_data:batch_x, y_true:batch_y, hold_prob:0.8})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('Currently on step {}'.format(i))\n",
    "            print('Test Set Loss:')\n",
    "            \n",
    "            # get random sample of 5000 from test set\n",
    "            test_indexes = np.random.randint(0, len(test_comments)-1, size=5000)\n",
    "            \n",
    "            test_comment_matrix_5000 = [test_comment_matrix[i] for i in test_indexes]\n",
    "            test_comment_matrix_5000 = np.reshape(test_comment_matrix_5000, [5000, -1])\n",
    "            \n",
    "            test_true_5000 = [test_true_matrix[i] for i in test_indexes]\n",
    "            test_true_5000 = np.reshape(test_true_5000, [5000, -1])\n",
    "            \n",
    "            print(sess.run(loss,feed_dict={input_data:test_comment_matrix_5000, y_true:test_true_5000, hold_prob:1.0}))\n",
    "            print('\\n')\n",
    "     \n",
    "    print('Finished training, making predictions...')   \n",
    "    \n",
    "    # overwrite train data to save memory\n",
    "    data = None\n",
    "    submission_data = CommentData(submission_comments, None)\n",
    "    \n",
    "    # clear test matrix\n",
    "    test_comment_matrix = None\n",
    "    \n",
    "    last_submission_ix = 0\n",
    "    \n",
    "    while submission_data.do_next_batch == True:\n",
    "        \n",
    "        batch_x = submission_data.next_batch(batchSize)\n",
    "        \n",
    "        batch_x = comment_embedder.transform(batch_x)\n",
    "        \n",
    "        if submission_data.i%10000 == 0:\n",
    "            print('On prediction {}'.format(submission_data.i))\n",
    "\n",
    "        submission_pred = sess.run(predict,feed_dict={input_data:batch_x, hold_prob:1.0})\n",
    "        \n",
    "        try:\n",
    "            pred_df\n",
    "        except NameError:\n",
    "            pred_df = pd.DataFrame(data=submission_pred, index=submission_df['id'][:submission_data.i], columns=labels)\n",
    "            last_submission_ix = submission_data.i\n",
    "        else:\n",
    "            pred_df = pred_df.append(pd.DataFrame(data=submission_pred, index=submission_df['id'][last_submission_ix:submission_data.i], columns=labels))\n",
    "            last_submission_ix = submission_data.i\n",
    "\n",
    "    pred_df.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
